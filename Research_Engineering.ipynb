{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMYXP2klQmquDRfBuEflIWf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kasattack567/Computer-Science/blob/main/Research_Engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction:\n",
        "\n",
        "\n",
        "## **Task**:\n",
        "\n",
        "Based on the text in these papers, implement the function (in Python, C++ or Rust),\n",
        "which accepts three lists of integer values:\n",
        "\n",
        "• Round numbers (correspond to n and r in the papers, respectively)\n",
        "\n",
        "• Number of trials (shots) for each data point\n",
        "\n",
        "• Number of trials (errors) of unsuccessful decoding among these shots\n",
        "\n",
        "The fraction errors[r]/shots[r] correspond to E(r) in the papers.  Your function should\n",
        "return a single floating-point value ε.\n",
        "\n",
        "\n",
        "## **Method:**\n",
        "\n",
        "The function calculates the logical error per round by analysing how fidelity decays with the number of error-correction rounds. It first determines the error rate from the input data and derives the corresponding fidelity. The natural logarithm of fidelity is then computed to linearise its exponential decay. A linear regression is performed on the transformed data to extract the slope, from which epsilon is calculated. The quality of the fit is assessed using the R^2 value, and the results are visualised with a scatter plot and a fitted straight line to validate the analysis.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zbCdMB06-LaY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def get_ler_per_round(round_numbers, shots, errors) -> float:\n",
        "    \"\"\"\n",
        "    Calculate the logical error per round (ε) from quantum error correction data.\n",
        "\n",
        "    This function determines the logical error rate per round, ε, based on data obtained\n",
        "    from quantum error correction experiments. It assumes that the error rates decay\n",
        "    exponentially as a function of the number of rounds, and extracts ε by performing\n",
        "    a linear fit on the logarithm of the fidelity.\n",
        "\n",
        "    Parameters:\n",
        "        round_numbers (list[int]): A list of integers representing the number of rounds\n",
        "                                   (stabilizer measurement cycles) at which data was collected.\n",
        "        shots (list[int]): A list of integers representing the total number of trials (shots)\n",
        "                           performed for each corresponding round.\n",
        "        errors (list[int]): A list of integers representing the number of unsuccessful\n",
        "                            decoding events (errors) observed among the shots for each round.\n",
        "\n",
        "    Returns:\n",
        "        float: The logical error per round, ε, which quantifies the rate of error propagation\n",
        "               through the quantum error correction cycles.\n",
        "\n",
        "    Method:\n",
        "        1. Compute the error rate \\( E(n) = \\text{errors}[n] / \\text{shots}[n] \\).\n",
        "        2. Derive the fidelity \\( F(n) = 1 - 2E(n) \\), representing the probability of\n",
        "           error-free operation.\n",
        "        3. Discard the first data point, as per best practices in error correction experiments.\n",
        "        4. Compute the natural logarithm of the fidelity to linearize its exponential decay.\n",
        "        5. Perform a linear regression on \\( \\log(F(n)) \\) versus the round numbers to determine\n",
        "           the slope \\( B \\).\n",
        "        6. Extract the logical error per round using the relationship \\( \\epsilon = (1 - \\exp(B)) / 2 \\).\n",
        "\n",
        "    Notes:\n",
        "        - This function assumes that the fidelity decays exponentially with the number of rounds.\n",
        "        - Input data must be sufficient to perform a meaningful linear regression, with fidelity\n",
        "          values strictly positive.\n",
        "    \"\"\"\n",
        "    # Convert inputs to numpy arrays\n",
        "    round_numbers = np.array(round_numbers, dtype=float)\n",
        "    shots = np.array(shots, dtype=float)\n",
        "    errors = np.array(errors, dtype=float)\n",
        "\n",
        "    # Compute error rates E(n) = errors / shots\n",
        "    E = errors / shots\n",
        "    # Compute fidelity F(n) = 1 - 2E(n)\n",
        "    F = 1.0 - 2.0 * E\n",
        "\n",
        "    # Discard the first data point as per the paper\n",
        "    n_fit = round_numbers[1:]\n",
        "    F_fit = F[1:]\n",
        "\n",
        "    # Ensure F(n) > 0 for logarithm\n",
        "    if np.any(F_fit <= 0):\n",
        "        raise ValueError(\"Fidelity must be positive to take the logarithm.\")\n",
        "\n",
        "    # Compute log(Fidelity)\n",
        "    log_F = np.log(F_fit)\n",
        "\n",
        "    # Perform a linear fit: log(F(n)) = A + B * n\n",
        "    B, A = np.polyfit(n_fit, log_F, 1)\n",
        "\n",
        "    # Extract ε from the slope\n",
        "    epsilon = (1 - np.exp(B)) / 2.0\n",
        "\n",
        "    return epsilon\n",
        "\n",
        "#Example:\n",
        "# Input data\n",
        "round_numbers = [1, 3, 5, 7]\n",
        "shots = [50000, 50000, 50000, 50000]\n",
        "errors = [10000, 19600, 23056, 24300]\n",
        "\n",
        "# Run the function and display results\n",
        "epsilon = get_ler_per_round(round_numbers, shots, errors)\n",
        "print(f\"Logical Error per Round (ε): {epsilon:.6f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Le--tzD6qTq",
        "outputId": "82f7643a-d221-490c-b351-6ec720912f20"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logical Error per Round (ε): 0.199983\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Explanation of Output Precision**\n",
        "\n",
        "The output of the function, **Logical Error per Round (ε): 0.199983**, is slightly different from the expected value of 0.2 due to the following factors:\n",
        "\n",
        "1. **Finite Precision of Floating-Point Arithmetic**:  \n",
        "   The function performs multiple operations, including division, logarithmic transformation, and exponentiation. Each of these introduces small numerical errors because floating-point numbers in Python have finite precision (typically 64-bit IEEE 754 representation). As a result, the calculations may not produce a perfectly exact value like 0.2.\n",
        "\n",
        "2. **Linear Regression Approximation**:  \n",
        "   The function fits a straight line to the logarithm of the fidelity data using `numpy.polyfit`, which minimizes the least-squares error. While the fit is very accurate, small deviations can arise due to finite numerical precision and slight variations in the input data.\n",
        "\n",
        "3. **Input Data and Rounding**:  \n",
        "   The provided input values for errors and shots lead to calculated error rates and fidelities that are not perfectly aligned with an exact \\( \\epsilon = 0.2 \\). Even small variations in the input propagate through the calculations and affect the final result.\n",
        "\n",
        "In summary, the small deviation from 0.2 arises from the combined effects of floating-point precision, numerical approximations during linear regression, and the inherent behavior of the input data. This level of precision is typical in scientific computations and does not affect the overall accuracy or usefulness of the result.\n"
      ],
      "metadata": {
        "id": "3JGppc1wls75"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eNTumNaQA1MF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}